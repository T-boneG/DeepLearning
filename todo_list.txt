Goals:

make deep_learning a consolidated package (module)
make it object oriented
write some nice examples in a separate examples directory that imports this deep_learning package


Question: is compute cost a negligible part of the compute time? Should it be able to be toggled on / off?


For tanh:
W = np.random.randn(<shape>) * np.sqrt(1 / n[l - 1])

For ReLU:
W = np.random.randn(<shape>) * np.sqrt(2 / n[l - 1])

gradient check:
epsilon = 10 ** -7
10 ** -7 (great!)
10 ** -3 (worry!)


Continue:
-TEST!!!
-gradient checking
-mini batch
-momentum
-adam (same as momentum but change values of beta1, beta2)
-learning rate decay
-create examples for: logistic regression, softmax, linear regression



Minibatch Gradient Descrent with Momentum
 - Common values for beta range from 0.8 to 0.999.
 - If you don't feel inclined to tune this, beta=0.9 is often a reasonable default.
 - TODO add default values and warning for weird values (and maybe option to suppress warnings)




